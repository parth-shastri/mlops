{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfx.__version__)\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = r\"D:/Users/shast/PycharmProjects/mlops/data\"\n",
    "PIPELINE_NAME = \"penguin-pipeline\"\n",
    "PIPELINE_ROOT = os.path.join(BASE_DIR, \"pipelines\", PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join(BASE_DIR, \"metadata\", PIPELINE_NAME, \"metadata.db\")\n",
    "SERVING_MODEL_DIR = os.path.join(BASE_DIR, \"serving_models\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\\\\\Users\\\\\\\\shast\\\\\\\\PycharmProjects\\\\\\\\mlops\\\\\\\\data\\\\tfx_datasetw51vawhf\\\\data.csv',\n",
       " <http.client.HTTPMessage at 0x1b4f9c0f250>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = tempfile.mkdtemp(prefix=os.path.join(BASE_DIR, \"tfx_dataset\"))\n",
    "_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_path, _data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this on the Linux terminal to remove the NA values from the data.csv\n",
    "# %%bash sed -i '/\\bNA\\b/d' {_data_filepath}\n",
    "# %%bash head {_data_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\\\\\Users\\\\\\\\shast\\\\\\\\PycharmProjects\\\\\\\\mlops\\\\\\\\data\\\\schema\\\\schema.pbtxt',\n",
       " <http.client.HTTPMessage at 0x1b4f7b11ac0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take existing schema instead of generating one with a different pipeline\n",
    "\n",
    "SCHEMA_PATH = os.path.join(BASE_DIR, \"schema\")\n",
    "\n",
    "_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "urllib.request.urlretrieve(_schema_uri, _schema_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_module_file = \"D:/Users/shast/PycharmProjects/mlops/main_tfx.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting D:/Users/shast/PycharmProjects/mlops/main_tfx.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_module_file}\n",
    "\n",
    "\n",
    "from typing import List, Text\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "import tfx.v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "\n",
    "_FEATURE_KEYS = [\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "_LABEL_KEY = \"species\"\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"This function is called by the TFX Transform Library internally.\"\"\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    for key in _FEATURE_KEYS:\n",
    "        outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "    \n",
    "    table_keys = [\"Adelie\", \"Chinstrap\", 'Gentoo']\n",
    "\n",
    "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=table_keys,\n",
    "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "        key_dtype = tf.string,\n",
    "        value_dtype = tf.int64\n",
    "    )\n",
    "\n",
    "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "    outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _apply_preprocessing(raw_features, tft_layer):\n",
    "\n",
    "    transformed_features = tft_layer(raw_features)\n",
    "\n",
    "    if _LABEL_KEY in raw_features:\n",
    "        transformed_label = transformed_features.pop(_LABEL_KEY)\n",
    "        return transformed_features, transformed_label\n",
    "\n",
    "    else:\n",
    "        return transformed_features, None\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "\n",
    "        required_feature_spec = {\n",
    "            k: v for k, v feature_spec.items() if k in _FEATURE_KEYS\n",
    "        }\n",
    "\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, required_feature_spec)\n",
    "\n",
    "        transformed_features, _ = _apply_preprocessing(parsed_features, model.tft_layer)\n",
    "\n",
    "        return model(transformed_features)\n",
    "\n",
    "\n",
    "return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(\n",
    "    file_pattern: List[Text],\n",
    "    data_accessor: tfx.components.DataAccessor,\n",
    "    tf_transform_output: tft.TFTransformOutput,\n",
    "    batch_size: int = 200\n",
    ") -> tf.data.Dataset\n",
    "\n",
    "dataset = data_accessor.tf_dataset_factory(\n",
    "    file_pattern,\n",
    "    tfxio.TensorflowdatasetOptions(batch_size=batch_size),\n",
    "    schema=tf_transform_output.raw_metadata.schema\n",
    ")\n",
    "\n",
    "transform_layer = tf_transform_output.get_features_layer()\n",
    "\n",
    "def apply_transform(raw_features):\n",
    "    return _apply_preprocessing(raw_features, transform_layer)\n",
    "\n",
    "\n",
    "return datase.map(apply_transform).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "    \"Build and define the keras model that will be used for training\"\n",
    "\n",
    "\n",
    "    inputs = [\n",
    "        tf.keras.layers.Input(shape=(1, ), name=key) for key in _FEATURE_KEYS\n",
    "    ]\n",
    "\n",
    "    x = tf.keras.layers.concatenate(inputs=inputs)\n",
    "\n",
    "    for _ in range(2):\n",
    "        x = tf.keras.layers.Dense(8)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(3)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-2),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"This function will be called by the Trainer Component\"\n",
    "\n",
    "    tf_transform_output = tft.TransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        batch_size=_TRAIN_BATCH_SIZE\n",
    "\n",
    "    )\n",
    "\n",
    "    val_dataset = _input_fn(\n",
    "        fn_args.train_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        batch_size=_EVAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model = _build_keras_model()\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fun_args.train_steps,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=fn_args.eval_steps\n",
    "    )\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output)\n",
    "    }\n",
    "\n",
    "    model.save(fn_args.seving_model_dir, save_format=\"tf\", signatures=signatures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('mlops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84cf66d1f64d0a53eb98f8338a0b7196efc60526c73417af3020714d5461c826"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
